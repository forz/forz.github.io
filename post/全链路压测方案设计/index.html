<!DOCTYPE html>
<html lang="zh-cn" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>全链路压测方案设计 | Forz Blog</title>
<meta name="keywords" content="架构" />
<meta name="description" content="前言 在阿里淘宝 双 11 的过程中，长期以来都是在生产环节做全链路压测的，通过实践我们发现在生产环境中做压测，实际上会和一个 IT 组织的结构、成熟度、流">
<meta name="author" content="">
<link rel="canonical" href="/post/%E5%85%A8%E9%93%BE%E8%B7%AF%E5%8E%8B%E6%B5%8B%E6%96%B9%E6%A1%88%E8%AE%BE%E8%AE%A1/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.00d5d4fc479b1575183ee8d86b4fb372ba9d9b1904e96fa8e4c40ff7debe2b94.css" integrity="sha256-ANXU/EebFXUYPujYa0&#43;zcrqdmxkE6W&#43;o5MQP996&#43;K5Q=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="mask-icon" href="/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.87.0" />
<meta property="og:title" content="全链路压测方案设计" />
<meta property="og:description" content="前言 在阿里淘宝 双 11 的过程中，长期以来都是在生产环节做全链路压测的，通过实践我们发现在生产环境中做压测，实际上会和一个 IT 组织的结构、成熟度、流" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/%E5%85%A8%E9%93%BE%E8%B7%AF%E5%8E%8B%E6%B5%8B%E6%96%B9%E6%A1%88%E8%AE%BE%E8%AE%A1/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-07-24T11:54:07&#43;00:00" />
<meta property="article:modified_time" content="2021-07-24T11:54:07&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="全链路压测方案设计"/>
<meta name="twitter:description" content="前言 在阿里淘宝 双 11 的过程中，长期以来都是在生产环节做全链路压测的，通过实践我们发现在生产环境中做压测，实际上会和一个 IT 组织的结构、成熟度、流"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "/post/"
    }
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "全链路压测方案设计",
      "item": "/post/%E5%85%A8%E9%93%BE%E8%B7%AF%E5%8E%8B%E6%B5%8B%E6%96%B9%E6%A1%88%E8%AE%BE%E8%AE%A1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "全链路压测方案设计",
  "name": "全链路压测方案设计",
  "description": "前言 在阿里淘宝 双 11 的过程中，长期以来都是在生产环节做全链路压测的，通过实践我们发现在生产环境中做压测，实际上会和一个 IT 组织的结构、成熟度、流",
  "keywords": [
    "架构"
  ],
  "articleBody": "前言 在阿里淘宝 双 11 的过程中，长期以来都是在生产环节做全链路压测的，通过实践我们发现在生产环境中做压测，实际上会和一个 IT 组织的结构、成熟度、流程等紧密相关，所以我们把全链路压测从简单的制作范围内脱离出来，变成整个业务连续性的方案。\n本文分四个方面为大家阐述：第一，整个全链路压测的意义，为什么要在生产环节上做全链路压测；第二，关于落地的技术点和解决方案；第三，生产过程中做全链路压测流程上的建议，考虑到每个组织的承受度不一样，给大家提供一些建议；第四，如何在第三方实现整个在生产环境中做业务连续性包括压测的结果。\n全链路压测的意义 上图显示了三个问题，实际上是不同的 IT 组织在和测试交流的时候，这三个问题是比较有代表性的。\n 很多测试同行说他们线下也做过性能测试，但是到了线上之后还是存在很多问题，因为不太可能会在线下模拟一个跟线上 1：1 的环境。在有很多第三方接口的情况下，大家也很少会去模拟线上整个场景。因此我们在线下做了很多测试工作后，总结出了为什么很多从线下容量推导到线上容量的公司却最终效果不是很好，就是这样的原因。 现在所有的 IT 组织都在搞 DevOps，我们的功能从一个月迭代一次到现在一周迭代一次，留给测试的时间越来越短。功能测试时间从之前的一周、两周缩短到现在三四天、两三天的时间，那性能测试就没有办法按时上线，很有可能会出现各种各样的性能问题，这会直接影响到企业的品牌影响力。 平时线上水位比较低，很少达到高峰期，但是会出现一些突发情况。比如像去年的疫情使得很多公司的业务变成在线业务。比如教育行业，之前是课堂上老师面对面的教育，现在选择线上在线平台来做，这类突发的情况会使测试工程师，包括开发运维团队受到很大的困扰。在这之前我先介绍一个概念，这个概念是由《黑天鹅》的原作者 Nassim Nicholas Taleb 提出，概念中心是脆弱与反脆弱。  什么是脆弱？脆弱就像玻璃，大家知道玻璃很脆易碎。脆弱的反义词是什么？不是强韧也不是坚韧，可能是反脆弱。什么是反脆弱呢？比如乒乓球，大家知道乒乓球在地上不用很大的力就可以破坏掉，踩一脚就破坏掉了，但是高速运动的情况下，乒乓球我们施加的力度越大，它的反弹力度越大，说明乒乓球在运动过程中有反脆弱的特性。\n我们的 IT 系统实际上也是这样的。不管什么代码都不能保证是完全没有问题的，我们的基础设施可能也是脆弱的，像服务器、数据库等总会有局限。我们的框架也总是脆弱的，将这些问题综合在一起，我们希望通过某些手段，比如通过预案、风险的识别，或者通过一些熔断的手段，最终把这些东西组合在一起，让整个 IT 系统有反脆弱的特性。总之，我们希望通过一些手段使得 IT 系统有足够的冗余，而且有足够多的预案应对突发的不确定性风险。\n如何打造 IT 系统反脆弱能力呢？我们希望通过一些手段，比如说像线上的压测能力，提供不确定的因素，接着通过在这个过程中实时监控，包括预案的能力，最终把这些不确定性的因素识别出来，并且在线上生产压测过程中对它做一些处理，更多可能会通过事后复盘等方式，做到对不确定性因素的识别。接着我们可能会在生产环境中通过之前的手段，在生产环境上做一个稳定性的常态化压测，实现长期稳定的场景，最终我们可能达到反脆弱能力所需要的整体监控的能力、运营防护能力，以及管控路由能力，这会让整个 IT 系统具备反脆弱的特性。\n全链路压测解决方案 到底如何在生产环境上做这样的全链路压测？它需要哪些技术手段？\n压测进程演变 一般情况下，测试是怎么样从线下一点点往线上演变的？我把它分为四个阶段：\n 目前绝大多数 IT 可以做到的是线下单系统压测，即针对单个接口或者单个场景做压测，同时也会做系统分析和性能分析。但在复杂的业务场景之下，我们可能没办法去充分发现问题，很多都是由开发或者测试同学自发进行的活动。 我们成立了一个类似于测试实验室或者测试组织的机构，这样一个大的部门可能会构造出一批类似于生产环境的性能测试环境，在这上面我们可能会做更多的事情，比如说做一个线下环境的全链路压测，并且我们可以根据之前积累的经验在上面做一些线下的回归，包括性能的诊断等。其实这一步相当于整个测试往前再走一步，对测试环境中的链路做一些分析，在上面演变一些能力，比如说风险的控制等等。 目前绝大部分 IT 企业和互联网企业愿意尝试线上生产环境的业务压测。这部分实际上和之前的第二阶段相差不多，但是在这个过程中人为的把它分为了两层：第一层是单纯的做全链路压测，很多 IT 公司已经在非生产环节中做了只读业务的压测，因为这样不会对数据造成污染。而再往下一层 ，有些组织可能会在正常生产时段中做进一步的全链路压测，这种情况下我们就会要求这个组织拥有更高的能力。 比如说我们需要对整个压测流量做一些染色，能够区分出来正常的业务数据，正常的流量和非正常的压测流量，可能有的会做一些环境的隔离，而在业务生产期间内我们做生产的压测，需要考虑到整个流量的偏移、限流，包括熔断机制等。不管怎样做业务，可能都会对最终的生产业务造成一定的影响，真正出现问题的时候可能需要有快速的熔断机制。 做到压缩熔断渲染，包括对熔断的机制——有了这样的能力之后，最后一个阶段就是整个生产链路的全链路压测，包括读写，它就具备了基本能力。这个方面我们其实更多的是通过引入库表，加上技术手段，在这个生产上做全链路压测，包括读业务、写业务等，同时我们有系统故障演练和生产变更演练的能力，在这种情况下我们可能最终具备了数据隔离能力、监控隔离能力和日志隔离能力。  全链路压测关键技术 对于整个全链路压测来说，我们需要几个关键的技术：\n全链路流量染色 压测流量隔离，主要是通过构建压测环境来解决，如线下压测环境，或泳道化/Set 化建设，将压测流量与线上流程完全隔离。优点是压测流量与线上流量完全隔离，不会影响到线上用户。缺点：机器资源及维护成本高，且压测结果需要经过一定的换算，才能得线上容量，结果准确性存在一定的问题。目前公司内压测都是在线上集群上完成的，线上泳道化正在建设中。\n压测标记 压测标记就是最常见的压测流量染色的方式。\n对于 RPC 协议，会在请求的头部中增加一个 Key：Value 的字段作为压测标记。\n对于 HTTP 和其他协议，会在请求头，自动注入一个 Stress 标记(Key-Value) 。\n压测标记 Key:Value，其中 key 是固定的 Stress_Tag 值，但是每个压测任务都有唯一的 Stress_Value 值，主要用于解决压测数据冲突，以及性能问题定位。\n压测标记透传 目前公司内各个基础组件、存储组件，以及 RPC 框架都已经支持了压测标记的透传。其原理是将压测标记的 KV 值存入 Context 中，然后在所有下游请求中都带上该 Context，下游服务可以根据 Context 中压测标记完成对压测流量的处理。在实际业务中，代码改造也非常简单，只需要透传 Context 即可。\nGolang 服务：将压测标记写入 Context 中。\nPython 服务：利用 threading.local()存储线程 Context。\nJava 服务：利用 ThreadLocal 存储线程 Context。\n压测开关 为了解决线上压测安全问题，我们还引入了压测开关组件。\n每个服务每个集群，都有一个压测开关。只有打开压测开关时，压测流量才能流入到服务内，否则就会被底层微服务框架直接拒绝，业务层无感。\n在每个 IDC 区域，都会有一个全局的压测总开关。只有打开了这个全局压测开关，压测流量才被允许在这个 IDC 内流转。\n当线上出现压测问题，除了从源头关闭压测流量以外，关闭目标服务的压测开关，也能立即阻断压测流量。\n全链路的数据隔离 我们需要通过哪些手段，比如通过影子库，通过运维的同学做一个和生产上面同样的影子库，然后切到影子库上，或者在生产库上做一个相同的影子表，来做数据隔离。第一种方式安全度高一些，但是缺点在于我们用影子库的时候整个生产环境是不可用的。生产影子库不能完全模拟出整个线上的情况，因为影子表需要我们有更高的技术水平，能够保障整个链路可追踪，包括整个数据如果一旦出错数据恢复能力等等。\n  MySQL、MongoDB：影子表。SDK 判断是否是压测流量，若是则根据配置映射至新表名。配置策略有两种，一是读写影子表，二是读线上表、写影子表。\n  Redis：Redis Key 加上 Stress 前缀。如 Stress_Tag=Valuex，那么读写 Redis 的 Key=Valuex_Key。这样可以解决多个压测任务数据冲突的问题。压测结束后，只需要对 Prefix=Valuex 做清除或过期操作即可。\n  MQ：对于消息队列，Rhino 平台有两种策略。一是直接丢弃，然后针对消息队列的性能，单独进行压测；二是在 Header 中透传压测标记，Consumer 根据压测标记和业务需求，再做特殊处理。默认走丢弃策略，业务方可根据需求进行配置。\n  其他存储，如 ES，ClickHouse 等，都有压测集群。压测时，会将压测请求打到指定的压测集群中。\n  全链路风险管控机制 也就是风险熔断机制，一旦真的发现生产环境的线上压测对我们的业务造成了影响，我们需要通过一些规则或者其他的指标来自动触发风险熔断，包括管控等等这样的手段，不管是提供施压机的流量，还是把生产系统损坏的部分做业务隔离，这样的手段都是我们做生产过程中全链路压测的必要手段。\n全链路日志隔离 其实日志本身不会对全链路造成太大的影响，但是因为做数字化水平的提升，日志基本上是 BI 同学包括运营的同学对整个业务分析最重要的数据来源，如果我们不做日志隔离很有可能会对 BI 决策造成一定的影响，比如压测过程中我们会大量采用某个地域的流量对生产环境做访问，BI 的同学可能会通过日志分析发现某一个地区做大，导致他错误的运营决策，所以说对于生产过程中的全链路压测来说，我们需要在整个生产过程中做一定的日志隔离，区分出来正常的生产流量和压测流量之间的存储。\n服务Mock 对于调用链中不能压测的服务(敏感服务)，或者第三方服务，为了压测请求的完整性，就需要对这些服务进行 Mock。业界通用的 Mock 方案有：\n 修改业务代码，修改服务调用为空转代码。优点：实现成本低。缺点：返回值固定，代码 \u0026业务入侵高，推动困难。如要 Mock 位置比较靠下游，超出部门覆盖业务范围，推动就非常麻烦。 通用 Mock 服务。通用 MockServer，会根据不同用户配置不同 Mock 规则，执行对应的响应延时，并返回对应响应数据。优点：无代码入侵，业务方无感知。缺点：实现成本高。  由于字节整个公司都采用微服务架构，导致一次压测涉及链路都比较长，快速无业务入侵的 Mock。\n方式成为了首选。Rhino 平台是通过公司 Service Mesh 和 ByteMock 系统来实现了高效的，对业务透明的服务 Mock。\n压测执行前，Rhino 平台需要向 Service Mesh 注册染色转发规则，并向 Mock 服务注册 Mock 规则。然后在压测流量中注入 Mock 染色标记，才能完成服务 Mock：\na. 基于 Service Mesh 的染色流量转发。首先需要在压测流量中注入转发染色标记，并在 Service Mesh 中注册对应的转发规则。Service Mesh 检测到染色流量后，就会将其转发到指定的 Mock Server 上，如图。\nb. 基于 Mock Server 的请求规则匹配。首先在 Mock Server 上注册 Mock 规则，以及匹配的 Response 和响应时延。当 Mock Server 接收到请求后，会根据规则进行响应，如图。\n全链路压测和业务连续性平台核心功能 这部分是真正想作为全链路压测和业务连续性平台所需要的功能。\n 首先是有来自于全地域的压测流量工具，这个流量工具具备的功能包括全地域流量挖掘、流量改造相关的功能。 整个压测识别，包括影子存储一部分的功能。黄色的部分是正常流量，蓝色的部分是压测的流量，我们可能通过施压机的改造让蓝色的部分加入一些标识，通过 Agent 的技术，它可以标识出带有的流量，通过底层的 Agent 技术将这些落到相应的影子库或者影子表里去，或者是缓存的影子区里。 做熔断的规则管理，所以需要有合理的控制台，这里可能会做一些安装探针管理，包括整个架构的管理、库表的维护、规则的维护、熔断机制的维护等。 最后是真正的施压部分。这里可能会安装一些探针或者是 Agent，这些 Agent 的作用是能够让这些流量落到相应的影子表里去，还有是通过相应的监控指标，比如说我们的错误达到 1%，或者是检查时间超过了一定的阈值之后，Agent 会及时上报，通过规则配置起到限流的作用。  通过这套架构，我们现在可以做到目前比按照整体环境大约节省成本是 40% 左右，基本上对整个生产业务没有任何切入。\n全链路压测风险防控能力 下面来具体谈一谈如何做一个影子数据库，包括整个流量识别。\n橙色的部分是真正的压测流量，这部分我们会在施压机上做一个标识，现在是会加一个后缀。另外还会在服务器做 filter，它其实是拦截器，我们会拦截到流量里面相关的标识，然后把它做区分、做染色，并且做跟踪，每一个请求基本上可以真正做到在任何中间件以及项目堆里都是透明可见的。\n真正在压测过程中通过 Agent 字节码结束将它直接改写，将字节的条件替换成压缩的条件。当然要先把影子库建好，通过底层的追踪我们可以把相应的流量，如果数据库就会走得比较明确，之后我们会做流量的测试，看看是否比较明确，而且我们可以做到整个测试数据带有标识，一旦真的没有走到诊断里面去，我们也可以在正常的表里做删除，并且每一个经过的区域对我们来说都是可见的。\n通过这样的方式，目前绝大部分 IT 组织是分三个阶段，当然有一些非常成熟的是分为两个阶段：\n  在上线之前发现问题，大多是由线下的开发或者测试调试过程中发现问题，然后做到整个接口的优化，确保最后没有代码的问题，包括 DNS 问题。这类问题基本上是在线下的环境，开发的环境解决掉。\n  在部署过程中，我们会做第三方插件比如安全等等问题，但是目前随着容器的发展，开发部署环境会被逐渐淡化掉。\n  在线上真正做生产环境的压测，这部分可能会做容量规划或者是压测，其他像整个大环境，比如说 CDN 或者 DNS 问题，或者是整个线上系统容量评估等等问题。\n  这些是我们目前在整个测试生命周期里希望在各个阶段实现的目的。\n压测流程的建议 考虑到各个组织的成熟度不一样，我们提供的这些建议不一定适用于所有的 IT 组织，但大家可以根据自身情况参考一下。\n我们一般为第三方实施全链路压测，线上生产压测，会经历五个阶段：\n首先是和第三方一起梳理业务的阶段，我们会做以下几件事情：\n  根据过往系统使用情况评估业务系统的性能指标、容量指标；\n  对现有信息系统做系统架构的梳理，确定整个被染色流量的路径途径；\n  对压测时长，包括间隔等做沟通，确认相关的压测场景设计；\n  生产数据脱敏，如果有一部分涉及到生产数据可能会做生产数据的脱敏等相关工作。\n  这部分做完是做第二部分，对某些应用进行改造。比如说做流量打标工作，通过监控的流量确定业务系统，可能在业务系统里会做相关监控的接入，相关的第三方组件会进行 Mock，整个压测场景的创建会和第三方沟通好。包括流量表建设和预案等等接入。\n三是整个压测的过程，整个生产状态下的全链路压测，会对整个系统进行性能优化及容量评估。\n四是将线上全链路压测常态化，这里面会有一些事情，比如说限流、降级、混沌工程验收，包括生产发布的事情。\n五是对于整个活动做复盘，看应急预案是否生效，还有哪些地方需要优化，这是生产环节全链路压测的生命周期。\n我们现在做一些更深入的东西，整个开发过程中，目前大家都使用 DevOps，可能单接口的性能测试在过程中就已经用到了，目前我们给企业打造的都包含了接口级别的单机性能测试，使用单机测试工具，在发布过程中开始验收单接口的性能问题，保证这个接口发到线上不会有代码级别错误，最终会省掉集成压测包括测试环境中压测的步骤，直接走到线上压测这个过程。单接口阶段我们会支持相应主流的框架压测，目前我们也在不断做测试环境集群的压测支持，还是希望直接用户跳过这个步骤开始在线上直接做流量隔离的压测。\n上图是我们认为一个完整的业务连续性平台需要的功能。\n  压测流量发起的控制台，流量发起端，这块实际上是管理整个压测流量和场景设计；\n  流量隔离控制台，这部分希望能够做到统一切流，当出现问题的时候可以一下把压测流量切掉，统一路由；\n  压测过程中有整个流量监控包括系统监控；压测过程中对于整个应用的性能监控平台，包括链路监控、JVM 监控、组件监控等等；\n  真正的混沌工程，包括流控规则、隔离规则、降级规则等等平台，这里会维护相应的规则。\n  最终我们希望这个平台能够达到的目的是：随时随地以低成本实现全链路压测；对于运维平台可以进行周期性的故障演练，并把这种能力给运维团队，让他们随时随地发起变更；为整个上线活动包括大促做一些兜底，可以避免突发活动击穿。因为长期固化生产压测会为我们带来容量和水位的极限，在演练过程中进行预案的实施，突发过程中会有更好的手段去避免，去做防护。\n以阿里为例，现在基本上可以做到以按月为主，因为大家知道淘宝每个月都有活动，每年有三个大活动：6.18、双11、双12。我们目前可以做小的演练，以周为实施单位做 双11、双12 或者 6.18 的大促，而且我们可以很清晰的组织 BU 内或者跨 BU 的压测活动，并能够很明确扩容方案。\n参考 全链路压测体系建设方案的思考与实践\n字节跳动全链路压测 (Rhino) 的实践\n",
  "wordCount" : "6737",
  "inLanguage": "zh-cn",
  "datePublished": "2021-07-24T11:54:07Z",
  "dateModified": "2021-07-24T11:54:07Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/%E5%85%A8%E9%93%BE%E8%B7%AF%E5%8E%8B%E6%B5%8B%E6%96%B9%E6%A1%88%E8%AE%BE%E8%AE%A1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Forz Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="/" accesskey="h" title="Forz Blog (Alt + H)">Forz Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="/post/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      全链路压测方案设计
    </h1>
    <div class="post-meta">July 24, 2021
</div>
  </header> 
  <div class="post-content"><h2 id="前言">前言<a hidden class="anchor" aria-hidden="true" href="#前言">#</a></h2>
<p>在阿里淘宝 双 11 的过程中，长期以来都是在生产环节做全链路压测的，通过实践我们发现在生产环境中做压测，实际上会和一个 IT 组织的结构、成熟度、流程等紧密相关，所以我们把全链路压测从简单的制作范围内脱离出来，变成整个业务连续性的方案。</p>
<p>本文分四个方面为大家阐述：第一，整个全链路压测的意义，为什么要在生产环节上做全链路压测；第二，关于落地的技术点和解决方案；第三，生产过程中做全链路压测流程上的建议，考虑到每个组织的承受度不一样，给大家提供一些建议；第四，如何在第三方实现整个在生产环境中做业务连续性包括压测的结果。</p>
<h2 id="全链路压测的意义">全链路压测的意义<a hidden class="anchor" aria-hidden="true" href="#全链路压测的意义">#</a></h2>
<p><img loading="lazy" src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210724182628.png" alt=""  />
</p>
<p>上图显示了三个问题，实际上是不同的 IT 组织在和测试交流的时候，这三个问题是比较有代表性的。</p>
<ol>
<li>很多测试同行说他们线下也做过性能测试，但是到了线上之后还是存在很多问题，因为不太可能会在线下模拟一个跟线上 1：1 的环境。在有很多第三方接口的情况下，大家也很少会去模拟线上整个场景。因此我们在线下做了很多测试工作后，总结出了为什么很多从线下容量推导到线上容量的公司却最终效果不是很好，就是这样的原因。</li>
<li>现在所有的 IT 组织都在搞 DevOps，我们的功能从一个月迭代一次到现在一周迭代一次，留给测试的时间越来越短。功能测试时间从之前的一周、两周缩短到现在三四天、两三天的时间，那性能测试就没有办法按时上线，很有可能会出现各种各样的性能问题，这会直接影响到企业的品牌影响力。</li>
<li>平时线上水位比较低，很少达到高峰期，但是会出现一些突发情况。比如像去年的疫情使得很多公司的业务变成在线业务。比如教育行业，之前是课堂上老师面对面的教育，现在选择线上在线平台来做，这类突发的情况会使测试工程师，包括开发运维团队受到很大的困扰。在这之前我先介绍一个概念，这个概念是由《黑天鹅》的原作者 Nassim Nicholas Taleb 提出，概念中心是脆弱与反脆弱。</li>
</ol>
<p>什么是脆弱？脆弱就像玻璃，大家知道玻璃很脆易碎。脆弱的反义词是什么？不是强韧也不是坚韧，可能是反脆弱。什么是反脆弱呢？比如乒乓球，大家知道乒乓球在地上不用很大的力就可以破坏掉，踩一脚就破坏掉了，但是高速运动的情况下，乒乓球我们施加的力度越大，它的反弹力度越大，说明乒乓球在运动过程中有反脆弱的特性。</p>
<p>我们的 IT 系统实际上也是这样的。不管什么代码都不能保证是完全没有问题的，我们的基础设施可能也是脆弱的，像服务器、数据库等总会有局限。我们的框架也总是脆弱的，将这些问题综合在一起，我们希望通过某些手段，比如通过预案、风险的识别，或者通过一些熔断的手段，最终把这些东西组合在一起，让整个 IT 系统有反脆弱的特性。总之，我们希望通过一些手段使得 IT 系统有足够的冗余，而且有足够多的预案应对突发的不确定性风险。</p>
<p><img loading="lazy" src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210724182924.png" alt=""  />
</p>
<p>如何打造 IT 系统反脆弱能力呢？我们希望通过一些手段，比如说像线上的压测能力，提供不确定的因素，接着通过在这个过程中实时监控，包括预案的能力，最终把这些不确定性的因素识别出来，并且在线上生产压测过程中对它做一些处理，更多可能会通过事后复盘等方式，做到对不确定性因素的识别。接着我们可能会在生产环境中通过之前的手段，在生产环境上做一个稳定性的常态化压测，实现长期稳定的场景，最终我们可能达到反脆弱能力所需要的整体监控的能力、运营防护能力，以及管控路由能力，这会让整个 IT 系统具备反脆弱的特性。</p>
<h2 id="全链路压测解决方案">全链路压测解决方案<a hidden class="anchor" aria-hidden="true" href="#全链路压测解决方案">#</a></h2>
<p>到底如何在生产环境上做这样的全链路压测？它需要哪些技术手段？</p>
<h3 id="压测进程演变">压测进程演变<a hidden class="anchor" aria-hidden="true" href="#压测进程演变">#</a></h3>
<p><img loading="lazy" src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210724183038.png" alt=""  />
</p>
<p>一般情况下，测试是怎么样从线下一点点往线上演变的？我把它分为四个阶段：</p>
<ol>
<li>目前绝大多数 IT 可以做到的是线下单系统压测，即针对单个接口或者单个场景做压测，同时也会做系统分析和性能分析。但在复杂的业务场景之下，我们可能没办法去充分发现问题，很多都是由开发或者测试同学自发进行的活动。</li>
<li>我们成立了一个类似于测试实验室或者测试组织的机构，这样一个大的部门可能会构造出一批类似于生产环境的性能测试环境，在这上面我们可能会做更多的事情，比如说做一个线下环境的全链路压测，并且我们可以根据之前积累的经验在上面做一些线下的回归，包括性能的诊断等。其实这一步相当于整个测试往前再走一步，对测试环境中的链路做一些分析，在上面演变一些能力，比如说风险的控制等等。</li>
<li>目前绝大部分 IT 企业和互联网企业愿意尝试线上生产环境的业务压测。这部分实际上和之前的第二阶段相差不多，但是在这个过程中人为的把它分为了两层：第一层是单纯的做全链路压测，很多 IT 公司已经在非生产环节中做了只读业务的压测，因为这样不会对数据造成污染。而再往下一层 ，有些组织可能会在正常生产时段中做进一步的全链路压测，这种情况下我们就会要求这个组织拥有更高的能力。
比如说我们需要对整个压测流量做一些染色，能够区分出来正常的业务数据，正常的流量和非正常的压测流量，可能有的会做一些环境的隔离，而在业务生产期间内我们做生产的压测，需要考虑到整个流量的偏移、限流，包括熔断机制等。不管怎样做业务，可能都会对最终的生产业务造成一定的影响，真正出现问题的时候可能需要有快速的熔断机制。</li>
<li>做到压缩熔断渲染，包括对熔断的机制——有了这样的能力之后，最后一个阶段就是整个生产链路的全链路压测，包括读写，它就具备了基本能力。这个方面我们其实更多的是通过引入库表，加上技术手段，在这个生产上做全链路压测，包括读业务、写业务等，同时我们有系统故障演练和生产变更演练的能力，在这种情况下我们可能最终具备了数据隔离能力、监控隔离能力和日志隔离能力。</li>
</ol>
<h2 id="全链路压测关键技术">全链路压测关键技术<a hidden class="anchor" aria-hidden="true" href="#全链路压测关键技术">#</a></h2>
<p><img loading="lazy" src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210724183523.png" alt=""  />
</p>
<p>对于整个全链路压测来说，我们需要几个关键的技术：</p>
<h3 id="全链路流量染色">全链路流量染色<a hidden class="anchor" aria-hidden="true" href="#全链路流量染色">#</a></h3>
<p>压测流量隔离，主要是通过构建压测环境来解决，如线下压测环境，或泳道化/Set 化建设，将压测流量与线上流程完全隔离。优点是压测流量与线上流量完全隔离，不会影响到线上用户。缺点：机器资源及维护成本高，且压测结果需要经过一定的换算，才能得线上容量，结果准确性存在一定的问题。目前公司内压测都是在线上集群上完成的，线上泳道化正在建设中。</p>
<h4 id="压测标记">压测标记<a hidden class="anchor" aria-hidden="true" href="#压测标记">#</a></h4>
<p>压测标记就是最常见的压测流量染色的方式。</p>
<p>对于 RPC 协议，会在请求的头部中增加一个 Key：Value 的字段作为压测标记。</p>
<p>对于 HTTP 和其他协议，会在请求头，自动注入一个 Stress 标记(Key-Value) 。</p>
<p>压测标记 Key:Value，其中 key 是固定的 Stress_Tag 值，但是每个压测任务都有唯一的 Stress_Value 值，主要用于解决压测数据冲突，以及性能问题定位。</p>
<h4 id="压测标记透传">压测标记透传<a hidden class="anchor" aria-hidden="true" href="#压测标记透传">#</a></h4>
<p>目前公司内各个基础组件、存储组件，以及 RPC 框架都已经支持了压测标记的透传。其原理是将压测标记的 KV 值存入 Context 中，然后在所有下游请求中都带上该 Context，下游服务可以根据 Context 中压测标记完成对压测流量的处理。在实际业务中，代码改造也非常简单，只需要透传 Context 即可。</p>
<p>Golang 服务：将压测标记写入 Context 中。</p>
<p>Python 服务：利用 threading.local()存储线程 Context。</p>
<p>Java 服务：利用 ThreadLocal 存储线程 Context。</p>
<h4 id="压测开关">压测开关<a hidden class="anchor" aria-hidden="true" href="#压测开关">#</a></h4>
<p>为了解决线上压测安全问题，我们还引入了压测开关组件。</p>
<p>每个服务每个集群，都有一个压测开关。只有打开压测开关时，压测流量才能流入到服务内，否则就会被底层微服务框架直接拒绝，业务层无感。</p>
<p>在每个 IDC 区域，都会有一个全局的压测总开关。只有打开了这个全局压测开关，压测流量才被允许在这个 IDC 内流转。</p>
<p>当线上出现压测问题，除了从源头关闭压测流量以外，关闭目标服务的压测开关，也能立即阻断压测流量。</p>
<h3 id="全链路的数据隔离">全链路的数据隔离<a hidden class="anchor" aria-hidden="true" href="#全链路的数据隔离">#</a></h3>
<p>我们需要通过哪些手段，比如通过影子库，通过运维的同学做一个和生产上面同样的影子库，然后切到影子库上，或者在生产库上做一个相同的影子表，来做数据隔离。第一种方式安全度高一些，但是缺点在于我们用影子库的时候整个生产环境是不可用的。生产影子库不能完全模拟出整个线上的情况，因为影子表需要我们有更高的技术水平，能够保障整个链路可追踪，包括整个数据如果一旦出错数据恢复能力等等。</p>
<p><img loading="lazy" src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210725134636.png" alt=""  />
</p>
<ul>
<li>
<p>MySQL、MongoDB：影子表。SDK 判断是否是压测流量，若是则根据配置映射至新表名。配置策略有两种，一是读写影子表，二是读线上表、写影子表。</p>
</li>
<li>
<p>Redis：Redis Key 加上 Stress 前缀。如 Stress_Tag=Valuex，那么读写 Redis 的 Key=Valuex_Key。这样可以解决多个压测任务数据冲突的问题。压测结束后，只需要对 Prefix=Valuex 做清除或过期操作即可。</p>
</li>
<li>
<p>MQ：对于消息队列，Rhino 平台有两种策略。一是直接丢弃，然后针对消息队列的性能，单独进行压测；二是在 Header 中透传压测标记，Consumer 根据压测标记和业务需求，再做特殊处理。默认走丢弃策略，业务方可根据需求进行配置。</p>
</li>
<li>
<p>其他存储，如 ES，ClickHouse 等，都有压测集群。压测时，会将压测请求打到指定的压测集群中。</p>
</li>
</ul>
<h3 id="全链路风险管控机制">全链路风险管控机制<a hidden class="anchor" aria-hidden="true" href="#全链路风险管控机制">#</a></h3>
<p>也就是风险熔断机制，一旦真的发现生产环境的线上压测对我们的业务造成了影响，我们需要通过一些规则或者其他的指标来自动触发风险熔断，包括管控等等这样的手段，不管是提供施压机的流量，还是把生产系统损坏的部分做业务隔离，这样的手段都是我们做生产过程中全链路压测的必要手段。</p>
<h3 id="全链路日志隔离">全链路日志隔离<a hidden class="anchor" aria-hidden="true" href="#全链路日志隔离">#</a></h3>
<p>其实日志本身不会对全链路造成太大的影响，但是因为做数字化水平的提升，日志基本上是 BI 同学包括运营的同学对整个业务分析最重要的数据来源，如果我们不做日志隔离很有可能会对 BI 决策造成一定的影响，比如压测过程中我们会大量采用某个地域的流量对生产环境做访问，BI 的同学可能会通过日志分析发现某一个地区做大，导致他错误的运营决策，所以说对于生产过程中的全链路压测来说，我们需要在整个生产过程中做一定的日志隔离，区分出来正常的生产流量和压测流量之间的存储。</p>
<h3 id="服务mock">服务Mock<a hidden class="anchor" aria-hidden="true" href="#服务mock">#</a></h3>
<p>对于调用链中不能压测的服务(敏感服务)，或者第三方服务，为了压测请求的完整性，就需要对这些服务进行 Mock。业界通用的 Mock 方案有：</p>
<ul>
<li>修改业务代码，修改服务调用为空转代码。优点：实现成本低。缺点：返回值固定，代码 &amp;业务入侵高，推动困难。如要 Mock 位置比较靠下游，超出部门覆盖业务范围，推动就非常麻烦。</li>
<li>通用 Mock 服务。通用 MockServer，会根据不同用户配置不同 Mock 规则，执行对应的响应延时，并返回对应响应数据。优点：无代码入侵，业务方无感知。缺点：实现成本高。</li>
</ul>
<p>由于字节整个公司都采用微服务架构，导致一次压测涉及链路都比较长，快速无业务入侵的 Mock。</p>
<p>方式成为了首选。Rhino 平台是通过公司 Service Mesh 和 ByteMock 系统来实现了高效的，对业务透明的服务 Mock。</p>
<p>压测执行前，Rhino 平台需要向 Service Mesh 注册染色转发规则，并向 Mock 服务注册 Mock 规则。然后在压测流量中注入 Mock 染色标记，才能完成服务 Mock：</p>
<p>a. 基于 Service Mesh 的染色流量转发。首先需要在压测流量中注入转发染色标记，并在 Service Mesh 中注册对应的转发规则。Service Mesh 检测到染色流量后，就会将其转发到指定的 Mock Server 上，如图。</p>
<p>b. 基于 Mock Server 的请求规则匹配。首先在 Mock Server 上注册 Mock 规则，以及匹配的 Response 和响应时延。当 Mock Server 接收到请求后，会根据规则进行响应，如图。</p>
<p><img loading="lazy" src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210725135311.png" alt=""  />
</p>
<p><img loading="lazy" src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210725135317.png" alt=""  />
</p>
<h2 id="全链路压测和业务连续性平台核心功能">全链路压测和业务连续性平台核心功能<a hidden class="anchor" aria-hidden="true" href="#全链路压测和业务连续性平台核心功能">#</a></h2>
<p><img loading="lazy" src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210724183643.png" alt=""  />
</p>
<p>这部分是真正想作为全链路压测和业务连续性平台所需要的功能。</p>
<ol>
<li>首先是有来自于全地域的压测流量工具，这个流量工具具备的功能包括全地域流量挖掘、流量改造相关的功能。</li>
<li>整个压测识别，包括影子存储一部分的功能。黄色的部分是正常流量，蓝色的部分是压测的流量，我们可能通过施压机的改造让蓝色的部分加入一些标识，通过 Agent 的技术，它可以标识出带有的流量，通过底层的 Agent 技术将这些落到相应的影子库或者影子表里去，或者是缓存的影子区里。</li>
<li>做熔断的规则管理，所以需要有合理的控制台，这里可能会做一些安装探针管理，包括整个架构的管理、库表的维护、规则的维护、熔断机制的维护等。</li>
<li>最后是真正的施压部分。这里可能会安装一些探针或者是 Agent，这些 Agent 的作用是能够让这些流量落到相应的影子表里去，还有是通过相应的监控指标，比如说我们的错误达到 1%，或者是检查时间超过了一定的阈值之后，Agent 会及时上报，通过规则配置起到限流的作用。</li>
</ol>
<p>通过这套架构，我们现在可以做到目前比按照整体环境大约节省成本是 40% 左右，基本上对整个生产业务没有任何切入。</p>
<h2 id="全链路压测风险防控能力">全链路压测风险防控能力<a hidden class="anchor" aria-hidden="true" href="#全链路压测风险防控能力">#</a></h2>
<p><img loading="lazy" src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210724183923.png" alt=""  />
</p>
<p>下面来具体谈一谈如何做一个影子数据库，包括整个流量识别。</p>
<p>橙色的部分是真正的压测流量，这部分我们会在施压机上做一个标识，现在是会加一个后缀。另外还会在服务器做 filter，它其实是拦截器，我们会拦截到流量里面相关的标识，然后把它做区分、做染色，并且做跟踪，每一个请求基本上可以真正做到在任何中间件以及项目堆里都是透明可见的。</p>
<p>真正在压测过程中通过 Agent 字节码结束将它直接改写，将字节的条件替换成压缩的条件。当然要先把影子库建好，通过底层的追踪我们可以把相应的流量，如果数据库就会走得比较明确，之后我们会做流量的测试，看看是否比较明确，而且我们可以做到整个测试数据带有标识，一旦真的没有走到诊断里面去，我们也可以在正常的表里做删除，并且每一个经过的区域对我们来说都是可见的。</p>
<p>通过这样的方式，目前绝大部分 IT 组织是分三个阶段，当然有一些非常成熟的是分为两个阶段：</p>
<ol>
<li>
<p>在上线之前发现问题，大多是由线下的开发或者测试调试过程中发现问题，然后做到整个接口的优化，确保最后没有代码的问题，包括 DNS 问题。这类问题基本上是在线下的环境，开发的环境解决掉。</p>
</li>
<li>
<p>在部署过程中，我们会做第三方插件比如安全等等问题，但是目前随着容器的发展，开发部署环境会被逐渐淡化掉。</p>
</li>
<li>
<p>在线上真正做生产环境的压测，这部分可能会做容量规划或者是压测，其他像整个大环境，比如说 CDN 或者 DNS 问题，或者是整个线上系统容量评估等等问题。</p>
</li>
</ol>
<p>这些是我们目前在整个测试生命周期里希望在各个阶段实现的目的。</p>
<h2 id="压测流程的建议">压测流程的建议<a hidden class="anchor" aria-hidden="true" href="#压测流程的建议">#</a></h2>
<p>考虑到各个组织的成熟度不一样，我们提供的这些建议不一定适用于所有的 IT 组织，但大家可以根据自身情况参考一下。</p>
<p>我们一般为第三方实施全链路压测，线上生产压测，会经历五个阶段：</p>
<p>首先是和第三方一起梳理业务的阶段，我们会做以下几件事情：</p>
<ol>
<li>
<p>根据过往系统使用情况评估业务系统的性能指标、容量指标；</p>
</li>
<li>
<p>对现有信息系统做系统架构的梳理，确定整个被染色流量的路径途径；</p>
</li>
<li>
<p>对压测时长，包括间隔等做沟通，确认相关的压测场景设计；</p>
</li>
<li>
<p>生产数据脱敏，如果有一部分涉及到生产数据可能会做生产数据的脱敏等相关工作。</p>
</li>
</ol>
<p>这部分做完是做第二部分，对某些应用进行改造。比如说做流量打标工作，通过监控的流量确定业务系统，可能在业务系统里会做相关监控的接入，相关的第三方组件会进行 Mock，整个压测场景的创建会和第三方沟通好。包括流量表建设和预案等等接入。</p>
<p>三是整个压测的过程，整个生产状态下的全链路压测，会对整个系统进行性能优化及容量评估。</p>
<p>四是将线上全链路压测常态化，这里面会有一些事情，比如说限流、降级、混沌工程验收，包括生产发布的事情。</p>
<p>五是对于整个活动做复盘，看应急预案是否生效，还有哪些地方需要优化，这是生产环节全链路压测的生命周期。</p>
<p>我们现在做一些更深入的东西，整个开发过程中，目前大家都使用 DevOps，可能单接口的性能测试在过程中就已经用到了，目前我们给企业打造的都包含了接口级别的单机性能测试，使用单机测试工具，在发布过程中开始验收单接口的性能问题，保证这个接口发到线上不会有代码级别错误，最终会省掉集成压测包括测试环境中压测的步骤，直接走到线上压测这个过程。单接口阶段我们会支持相应主流的框架压测，目前我们也在不断做测试环境集群的压测支持，还是希望直接用户跳过这个步骤开始在线上直接做流量隔离的压测。</p>
<p><img loading="lazy" src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210724184923.png" alt=""  />
</p>
<p>上图是我们认为一个完整的业务连续性平台需要的功能。</p>
<ol>
<li>
<p>压测流量发起的控制台，流量发起端，这块实际上是管理整个压测流量和场景设计；</p>
</li>
<li>
<p>流量隔离控制台，这部分希望能够做到统一切流，当出现问题的时候可以一下把压测流量切掉，统一路由；</p>
</li>
<li>
<p>压测过程中有整个流量监控包括系统监控；压测过程中对于整个应用的性能监控平台，包括链路监控、JVM 监控、组件监控等等；</p>
</li>
<li>
<p>真正的混沌工程，包括流控规则、隔离规则、降级规则等等平台，这里会维护相应的规则。</p>
</li>
</ol>
<p>最终我们希望这个平台能够达到的目的是：随时随地以低成本实现全链路压测；对于运维平台可以进行周期性的故障演练，并把这种能力给运维团队，让他们随时随地发起变更；为整个上线活动包括大促做一些兜底，可以避免突发活动击穿。因为长期固化生产压测会为我们带来容量和水位的极限，在演练过程中进行预案的实施，突发过程中会有更好的手段去避免，去做防护。</p>
<p>以阿里为例，现在基本上可以做到以按月为主，因为大家知道淘宝每个月都有活动，每年有三个大活动：6.18、双11、双12。我们目前可以做小的演练，以周为实施单位做 双11、双12 或者 6.18 的大促，而且我们可以很清晰的组织 BU 内或者跨 BU 的压测活动，并能够很明确扩容方案。</p>
<h2 id="参考">参考<a hidden class="anchor" aria-hidden="true" href="#参考">#</a></h2>
<p><a href="https://blog.csdn.net/m0_37055174/article/details/118213319">全链路压测体系建设方案的思考与实践</a></p>
<p><a href="https://www.infoq.cn/article/o9i6bfecbm3ti5htfm1r">字节跳动全链路压测 (Rhino) 的实践</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="/tags/%E6%9E%B6%E6%9E%84/">架构</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2021 <a href="/">Forz Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
