<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>为什么容器内存占用居高不下，频频 OOM - Forz Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Forz" /><meta name="description" content="前言 最近我在回顾思考（写 PPT），整理了现状，发现了这个问题存在多时，经过一番波折，最终确定了元凶和相对可行的解决方案，因此也在这里分享一下" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.87.0 with theme even" />


<link rel="canonical" href="/post/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AE%B9%E5%99%A8%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E5%B1%85%E9%AB%98%E4%B8%8D%E4%B8%8B%E9%A2%91%E9%A2%91-oom/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="为什么容器内存占用居高不下，频频 OOM" />
<meta property="og:description" content="前言 最近我在回顾思考（写 PPT），整理了现状，发现了这个问题存在多时，经过一番波折，最终确定了元凶和相对可行的解决方案，因此也在这里分享一下" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AE%B9%E5%99%A8%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E5%B1%85%E9%AB%98%E4%B8%8D%E4%B8%8B%E9%A2%91%E9%A2%91-oom/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-06-07T18:49:05+00:00" />
<meta property="article:modified_time" content="2021-06-07T18:49:05+00:00" />

<meta itemprop="name" content="为什么容器内存占用居高不下，频频 OOM">
<meta itemprop="description" content="前言 最近我在回顾思考（写 PPT），整理了现状，发现了这个问题存在多时，经过一番波折，最终确定了元凶和相对可行的解决方案，因此也在这里分享一下"><meta itemprop="datePublished" content="2021-06-07T18:49:05+00:00" />
<meta itemprop="dateModified" content="2021-06-07T18:49:05+00:00" />
<meta itemprop="wordCount" content="5068">
<meta itemprop="keywords" content="k8s," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="为什么容器内存占用居高不下，频频 OOM"/>
<meta name="twitter:description" content="前言 最近我在回顾思考（写 PPT），整理了现状，发现了这个问题存在多时，经过一番波折，最终确定了元凶和相对可行的解决方案，因此也在这里分享一下"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Forz Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Forz Blog</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">为什么容器内存占用居高不下，频频 OOM</h1>

      <div class="post-meta">
        <span class="post-time"> 2021-06-07 </span>
        <div class="post-category">
            <a href="/categories/k8s/"> k8s </a>
            </div>
          <span class="more-meta"> 约 5068 字 </span>
          <span class="more-meta"> 预计阅读 11 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#前言">前言</a></li>
    <li><a href="#现象">现象</a>
      <ul>
        <li><a href="#内存居高不下">内存居高不下</a></li>
        <li><a href="#进入重启怪圈">进入重启怪圈</a></li>
      </ul>
    </li>
    <li><a href="#查看系统内存">查看系统内存</a></li>
    <li><a href="#猜想一频繁申请重复对象">猜想一：频繁申请重复对象</a>
      <ul>
        <li><a href="#syncpool">sync.Pool</a></li>
        <li><a href="#验证场景">验证场景</a></li>
      </ul>
    </li>
    <li><a href="#猜想二不知名内存泄露">猜想二：不知名内存泄露</a></li>
    <li><a href="#猜想三madvise-策略变更">猜想三：madvise 策略变更</a></li>
    <li><a href="#猜想四监控判别条件有问题">猜想四：监控/判别条件有问题</a></li>
    <li><a href="#猜想五容器环境的机制">猜想五：容器环境的机制</a></li>
    <li><a href="#原因">原因</a></li>
    <li><a href="#解决方案">解决方案</a>
      <ul>
        <li><a href="#开发角度">开发角度</a></li>
        <li><a href="#运维角度">运维角度</a></li>
      </ul>
    </li>
    <li><a href="#表象">表象</a></li>
    <li><a href="#思考">思考</a></li>
    <li><a href="#根因">根因</a>
      <ul>
        <li><a href="#问题版本">问题版本</a></li>
      </ul>
    </li>
    <li><a href="#解决方案-1">解决方案</a>
      <ul>
        <li><a href="#调整内核参数">调整内核参数</a></li>
        <li><a href="#升级内核版本">升级内核版本</a></li>
      </ul>
    </li>
    <li><a href="#总结">总结</a></li>
    <li><a href="#后话">后话</a></li>
    <li><a href="#转载">转载</a></li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <h2 id="前言">前言</h2>
<p>最近我在回顾思考（写 PPT），整理了现状，发现了这个问题存在多时，经过一番波折，最终确定了元凶和相对可行的解决方案，因此也在这里分享一下排查历程。</p>
<p>时间线：</p>
<ul>
<li>
<p>在上 Kubernetes 的前半年，只是用 Kubernetes，开发没有权限，业务服务极少，忙着写新业务，风平浪静。</p>
</li>
<li>
<p>在上 Kubernetes 的后半年，业务服务较少，偶尔会阶段性被运维唤醒，问之 “为什么你们的服务内存占用这么高，赶紧查”。此时大家还在为新业务冲刺，猜测也许是业务代码问题，但没有调整代码去尝试解决。</p>
</li>
<li>
<p>在上 Kubernetes 的第二年，业务服务逐渐增多，普遍增加了容器限额 Limits，出现了好几个业务服务是内存小怪兽，因此如果不限制的话，服务过度占用会导致驱逐，因此反馈语也就变成了：“为什么你们的服务内存占用这么高，老被 OOM Kill，赶紧查”。据闻也有几个业务大佬有去排查（因为 OOM 反馈），似乎没得出最终解决方案。</p>
</li>
</ul>
<p>不禁让我们思考，为什么个别 Go 业务服务，Memory 总是提示这么高，经常达到容器限额，以至于被动 OOM Kill，是不是有什么安全隐患？</p>
<h2 id="现象">现象</h2>
<h3 id="内存居高不下">内存居高不下</h3>
<p>发现个别业务服务内存占用挺高，触发告警，且通过 Grafana 发现在凌晨（没有什么流量）的情况下，内存占用量依然拉平，没有打算下降的样子，高峰更是不得了，像是个内存炸弹：</p>
<p><img src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210607185337.png" alt=""></p>
<p>并且我所观测的这个服务，早年还只是 100MB。现在随着业务迭代和上升，目前已经稳步 4GB，容器限额 Limits 纷纷给它开道，但我想总不能是无休止的增加资源吧，这是一个大问题。</p>
<h3 id="进入重启怪圈">进入重启怪圈</h3>
<p>有的业务服务，业务量小，自然也就没有调整容器限额，因此得不到内存资源，又超过额度，就会进入疯狂的重启怪圈：</p>
<p><img src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210607185352.png" alt=""></p>
<p>重启将近 300 次，非常不正常了，更不用提所接受到的告警通知。</p>
<h2 id="查看系统内存">查看系统内存</h2>
<p>一般系统内存过高的情况下，可以通过 free -m 查看当前系统的内存使用情况：</p>
<p><img src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210608113952.png" alt=""></p>
<p>可以看到真正意义上free的内存只有1365MB.</p>
<p>在发现是系统内存占用高后，就会有读者会提到，为什么不 “手动清理 Cache”，因为 Cache 高的话，可以通过 drop_caches 的方式来清理：</p>
<p>1.清理 page cache：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-s" data-lang="s"><span class="n">echo</span> <span class="m">1</span> <span class="o">&gt;</span> <span class="o">/</span><span class="n">proc</span><span class="o">/</span><span class="n">sys</span><span class="o">/</span><span class="n">vm</span><span class="o">/</span><span class="n">drop_caches</span>
</code></pre></td></tr></table>
</div>
</div><p>2.清理 dentries 和 inodes：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-s" data-lang="s"><span class="n">echo</span> <span class="m">2</span> <span class="o">&gt;</span> <span class="o">/</span><span class="n">proc</span><span class="o">/</span><span class="n">sys</span><span class="o">/</span><span class="n">vm</span><span class="o">/</span><span class="n">drop_caches</span>
</code></pre></td></tr></table>
</div>
</div><p>3.清理 page cache、dentries 和 inodes：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-s" data-lang="s"><span class="n">echo</span> <span class="m">3</span> <span class="o">&gt;</span> <span class="o">/</span><span class="n">proc</span><span class="o">/</span><span class="n">sys</span><span class="o">/</span><span class="n">vm</span><span class="o">/</span><span class="n">drop_caches</span>
</code></pre></td></tr></table>
</div>
</div><p>但新问题又出现了，因为我们的命题是在容器中，在 Kubernetes 中，若执行 drop_caches 相关命令，将会对 Node 节点上的所有其他应用程序产生影响，尤其是那些占用大量 IO 并由于缓冲区高速缓存而获得更好性能的应用程序，可能会产生 “负面” 后果。</p>
<p>我想这并不是一个好办法。</p>
<h2 id="猜想一频繁申请重复对象">猜想一：频繁申请重复对象</h2>
<p>出现问题的个别业务服务都有几个特点，那就是基本为图片处理类的功能，例如：图片解压缩、批量生成二维码、PDF 生成等，因此就怀疑是否在量大时频繁申请重复对象，而 Go 本身又没有及时释放内存，因此导致持续占用。</p>
<h3 id="syncpool">sync.Pool</h3>
<p>基本上想解决 “频繁申请重复对象”，我们大多会采用多级内存池的方式，也可以用最常见的 sync.Pool，这里可参考全成所借述的《Go 夜读》上关于 sync.Pool 的分享，关于这类情况的场景：</p>
<p>当多个 goroutine 都需要创建同⼀个对象的时候，如果 goroutine 数过多，导致对象的创建数⽬剧增，进⽽导致 GC 压⼒增大。形成 “并发⼤－占⽤内存⼤－GC 缓慢－处理并发能⼒降低－并发更⼤”这样的恶性循环。</p>
<h3 id="验证场景">验证场景</h3>
<p>在描述中关注到几个关键字，分别是并发大，Goroutine 数过多，GC 压力增大，GC 缓慢。也就是需要满足上述几个硬性条件，才可以认为是符合猜想的。</p>
<p>通过拉取 PProf goroutine，可得知 Goroutine 数并不高：</p>
<p><img src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210608112116.png" alt=""></p>
<p>另外在凌晨长达 6 小时，没有什么流量的情况下，也不符合并发大，Goroutine 数过多的情况，若要更进一步确认，可通过 Grafana 落实其量的高低。</p>
<p>从结论上来讲，我认为与其没有特别直接的关系，但猜想其所对应的业务功能到导致的间接关系应当存在。</p>
<h2 id="猜想二不知名内存泄露">猜想二：不知名内存泄露</h2>
<p>内存居高不下，其中一个反应就是猜测是否存在泄露，而我们的容器中目前只跑着一个 Go 进程，因此首要看看该 Go 应用是否有问题。这时候可以借助 PProf heap（可以使用 base -diff）：</p>
<p><img src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210608112245.png" alt=""></p>
<p>显然其提示的内存使用不高，那会不会是 PProf 出现了 BUG 呢。接下通过命令也可确定 Go 进程的 RSS 并不高，但 VSZ 却相对 “高” 的惊人，我在 19 年针对此写过一篇《Go 应用内存占用太多，让排查？（VSZ篇）》 ，这次 VSZ 过高也给我留下了一个念想。</p>
<p>从结论上来讲，也不像 Go 进程内存泄露的问题，因此也将其排除。</p>
<h2 id="猜想三madvise-策略变更">猜想三：madvise 策略变更</h2>
<ul>
<li>在 Go1.12 以前，Go Runtime 在 Linux 上使用的是 MADV_DONTNEED 策略，可以让 RSS 下降的比较快，就是效率差点。</li>
<li>在 Go1.12 及以后，Go Runtime 专门针对其进行了优化，使用了更为高效的 MADV_FREE 策略。但这样子所带来的副作用就是 RSS 不会立刻下降，要等到系统有内存压力了才会释放占用，RSS 才会下降。</li>
</ul>
<p>查看容器的 Linux 内核版本：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-s" data-lang="s"><span class="o">$</span> <span class="n">uname</span> <span class="o">-</span><span class="n">a</span>
<span class="n">Linux</span> <span class="n">xxx</span><span class="o">-</span><span class="n">xxx</span><span class="m">-99</span><span class="n">bd5776f</span><span class="o">-</span><span class="n">k9t8z</span> <span class="m">3.10.0-693.2.2</span><span class="n">.el7.x86_64</span>
</code></pre></td></tr></table>
</div>
</div><p>但 MADV_FREE 的策略改变，需要 Linux 内核在 4.5 及以上（详细可见 go/issues/23687），显然不符合，因此也将其从猜测中排除。</p>
<h2 id="猜想四监控判别条件有问题">猜想四：监控/判别条件有问题</h2>
<p>会不会是 Grafana 的图表错了，Kubernetes OOM Kill 的判别标准也错了呢，显然不大可能，毕竟我们拥抱云，阿里云 Kubernetes 也运行了好几年。</p>
<p><img src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210608112855.png" alt=""></p>
<p>但在这次怀疑中，我了解到 OOM 的判断标准是 container_memory_working_set_bytes 指标，因此有了下一步猜想。</p>
<h2 id="猜想五容器环境的机制">猜想五：容器环境的机制</h2>
<p>既然不是业务代码影响，也不是 Go Runtime 的直接影响，那是否与环境本身有关呢，我们可以得知容器 OOM 的判别标准是 container_memory_working_set_bytes（当前工作集）。</p>
<p>而 container_memory_working_set_bytes 是由 cadvisor 提供的，对应下述指标：</p>
<p><img src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210608112917.png" alt=""></p>
<p>从结论上来讲，Memory 换算过来是 4GB+，石锤。接下来的问题就是 Memory 是怎么计算出来的呢，显然和 RSS 不对标。</p>
<h2 id="原因">原因</h2>
<p>从 cadvisor/issues/638 可得知 container_memory_working_set_bytes 指标的组成实际上是 RSS + Cache。而 Cache 高的情况，常见于进程有大量文件 IO，占用 Cache 可能就会比较高，猜测也与 Go 版本、Linux 内核版本的 Cache 释放、回收方式有较大关系。</p>
<p><img src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210608112932.png" alt=""></p>
<p>而各业务模块常见功能，如：</p>
<ul>
<li>批量图片解压缩。</li>
<li>批量二维码生成。</li>
<li>批量上传渲染后图片。</li>
<li>批量 PDF 生成。</li>
<li>…</li>
</ul>
<p>只要是涉及有大量文件 IO 的服务，基本上是这个问题的老常客了，写这类服务基本写一个中一个，因为这是一个混合问题，像其它单纯操作为主的业务服务就很 “正常”，不会出现内存居高不下。</p>
<h2 id="解决方案">解决方案</h2>
<p>在本场景中 cadvisor 所提供的判别标准 container_memory_working_set_bytes 是不可变更的，也就是无法把判别标准改为 RSS，因此我们只能考虑掌握主动权。</p>
<h3 id="开发角度">开发角度</h3>
<p>使用类 sync.Pool 做多级内存池管理，防止申请到 “不合适”的内存空间，常见的例子： ioutil.ReadAll：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="p">(</span><span class="nx">b</span> <span class="o">*</span><span class="nx">Buffer</span><span class="p">)</span> <span class="nf">ReadFrom</span><span class="p">(</span><span class="nx">r</span> <span class="nx">io</span><span class="p">.</span><span class="nx">Reader</span><span class="p">)</span> <span class="p">(</span><span class="nx">n</span> <span class="kt">int64</span><span class="p">,</span> <span class="nx">err</span> <span class="kt">error</span><span class="p">)</span> <span class="p">{</span>
    <span class="err">…</span>
    <span class="k">for</span> <span class="p">{</span>
        <span class="k">if</span> <span class="nx">free</span> <span class="o">:=</span> <span class="nb">cap</span><span class="p">(</span><span class="nx">b</span><span class="p">.</span><span class="nx">buf</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="nx">b</span><span class="p">.</span><span class="nx">buf</span><span class="p">);</span> <span class="nx">free</span> <span class="p">&lt;</span> <span class="nx">MinRead</span> <span class="p">{</span>
            <span class="nx">newBuf</span> <span class="o">:=</span> <span class="nx">b</span><span class="p">.</span><span class="nx">buf</span>
            <span class="k">if</span> <span class="nx">b</span><span class="p">.</span><span class="nx">off</span><span class="o">+</span><span class="nx">free</span> <span class="p">&lt;</span> <span class="nx">MinRead</span> <span class="p">{</span>
                    <span class="nx">newBuf</span> <span class="p">=</span> <span class="nf">makeSlice</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="nb">cap</span><span class="p">(</span><span class="nx">b</span><span class="p">.</span><span class="nx">buf</span><span class="p">)</span> <span class="o">+</span> <span class="nx">MinRead</span><span class="p">)</span>  <span class="c1">// 扩充双倍空间
</span><span class="c1"></span>                    <span class="nb">copy</span><span class="p">(</span><span class="nx">newBuf</span><span class="p">,</span> <span class="nx">b</span><span class="p">.</span><span class="nx">buf</span><span class="p">[</span><span class="nx">b</span><span class="p">.</span><span class="nx">off</span><span class="p">:])</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><p>核心是做好做多级内存池管理，因为使用多级内存池，就会预先定义多个 Pool，比如大小 100，200，300的 Pool 池，当你要 150 的时候，分配200，就可以避免部分的内存碎片和内存碎块。</p>
<p>但从另外一个角度来看这存在着一定的难度，因为你怎么知道什么时候在哪个集群上会突然出现这类型的服务，何况开发人员的预期情况参差不齐，写多级内存池写出 BUG 也是有可能的。</p>
<p>让业务服务无限重启，也是不现实的，被动重启，没有控制，且告警，存在风险。</p>
<h3 id="运维角度">运维角度</h3>
<p>为了掌握主动权，我们可以在部署环境可以配合脚本做 “手动” HPA，当容器内存指标超过约定限制后，起一个新的容器替换，再将原先的容器给释放掉，就可以在预期内替换且业务稳定了。</p>
<p><img src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210608113229.png" alt=""></p>
<h2 id="表象">表象</h2>
<p>回归原始，那就是为什么要排查这个问题，本质原因就是容器设置了 Memory Limits，而容器在运行中达到了 Limits 上限，被 OOM 掉了，所以我们想知道为什么会出现这个情况。</p>
<p>在前文中我们针对了五大类情况进行了猜想：</p>
<ul>
<li>频繁申请重复对象。</li>
<li>不知名内存泄露。</li>
<li>madvise 策略变更。</li>
<li>监控/判别条件有问题。</li>
<li>容器环境的机制。</li>
</ul>
<p>在逐一排除后，后续发现容器的 Memory OOM 判定标准是 container_memory_working_set_bytes 指标，其实际组成为 RSS + Cache（最近访问的内存、脏内存和内核内存）。</p>
<p>在排除进程内存泄露的情况下，我们肯定是希望知道 Cache 中有什么，为什么占用了那么大的空间，此时我们可以通过 Linux pmap 来查看该容器进程的内存映射情况：</p>
<p><img src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210608113518.png" alt=""></p>
<p>在上图中，我们发现了大量的 mapping 为 anon 的内存映射，最终 totals 确实达到了容器 Memory 相当的量，那么 anon 又是什么呢。实质上 anon 行表示在磁盘上没有对应的文件，也就是没有实际存在的载体，是 anonymous。</p>
<h2 id="思考">思考</h2>
<p>既然存在如此多的 anon，结合先前的考虑，我们知道出现这种情况的服务都是文件处理型服务，包含大量的批量生成图片、生成 PDF 等资源消耗型的任务，也就是会瞬间申请大量的内存，使得系统的空闲内存触及全局最低水位线（global wmark_min），而在触及全局最低水位线后，会尝试进行回收，实在不行才会触发 cgroup OOM 的行为。</p>
<p>那么更进一步思考的是两个问题，一个是 cgroup 达到 Limits 前的尝试释放仍然不足以支撑所需申请的连续内存段，而另外一个问题就是为什么 Cache 并没有释放：</p>
<p><img src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210608121829.png" alt=""></p>
<p>通过上图，可以肯定该服务在凌晨 00：00-06：00 是没有什么流量的，但是 container_memory_working_set_bytes 指标依旧稳定不变，排除 RSS 的原因，那配合指标的查看基本确定是该 cgroup 的 Cache 没有释放。</p>
<p>而 Cache 的占用高，主要考虑是由于其频繁操作文件导致，因为在 Linux 中，在第一次读取文件时会将一份放到系统 Cache，另外一份则放入进程内存中使用。关键点在于当进程运行完毕关闭后，系统 Cache 是不会马上回收的，需要经过系统的内存管理后再适时释放。</p>
<p>cache是操作系统内核的机制，同容器/业务/时间点无关，并不会说到了凌晨自动释放。</p>
<p>另外，如果压力降下来之后，只要系统未处于内存紧缺的状态，这部分cache是不会释放的。类似于拿内存换文件的io性能。毕竟【空闲】的内存放在那里，不用就浪费了。</p>
<p><img src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210608175244.png" alt=""></p>
<p>可以看到，节点上真正意义上free的内存只有263M，但available的内存有13G.available的内存包含了这里的free和部分buff/cache，当需要时，是可以随时用的.</p>
<p>但我们发现 Cache 的持续不释放，进程也没外部流量，RSS 也低的可怜，Cache 不像被进程占用住了的样子（这一步的排除很重要），最终就考虑到是否 Linux 内核在这块内存管理上存在 BUG 呢？</p>
<h2 id="根因">根因</h2>
<h3 id="问题版本">问题版本</h3>
<p>该服务所使用的 Kubernetes 是 1.11.5 版本，Linux 内核版本为 3.10.x，时间为 2017 年 9 月：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-s" data-lang="s"><span class="o">$</span> <span class="n">uname</span> <span class="o">-</span><span class="n">a</span>
<span class="n">Linux</span> <span class="n">xxxxx</span><span class="o">-</span><span class="n">xxx</span><span class="m">-99</span><span class="n">bd5776f</span><span class="o">-</span><span class="n">k9t8z</span> <span class="m">3.10.0-693.2.2</span><span class="n">.el7.x86_64</span> <span class="c1">#1 SMP Tue Sep 12 22:26:13 UTC 2017 x86_64 Linux</span>
</code></pre></td></tr></table>
</div>
</div><p>都算是有一定年代的老版本了。</p>
<h4 id="原因分析">原因分析</h4>
<p>memcg 是 Linux 内核中管理 cgroup 内存的模块，但实际上在 Linux 3.10.x 的低内核版本中存在不少实现上的 BUG，其中最具代表性的是 memory cgroup 中 kmem accounting 相关的问题（在低版本中属于 alpha 特性）：</p>
<ul>
<li>
<p>slab 泄露：具体可详见该文章 SLUB: Unable to allocate memory on node -1 中的介绍和说明。</p>
</li>
<li>
<p>memory cgroup 泄露：在删除容器后没有回收完全，而 Linux 内核对 memory cgroup 的总数限制是 65535 个，若频繁创建删除开启了 kmem 的 cgroup，就会导致无法再创建新的 memory cgroup。</p>
</li>
</ul>
<p>当然，为什么出现问题后绝大多数是由 Kubernetes、Docker 的相关使用者发现的呢（从 issues 时间上来看），这与云原生的兴起，这类问题与内部容器化的机制相互影响，最终开发者 “发现” 了这类应用频繁出现 OOM，于是开始进行排查。</p>
<h2 id="解决方案-1">解决方案</h2>
<h3 id="调整内核参数">调整内核参数</h3>
<p>关闭 kmem accounting：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-s" data-lang="s"><span class="n">cgroup.memory</span><span class="o">=</span><span class="n">nokmem</span>
</code></pre></td></tr></table>
</div>
</div><p>也可以通过 kubelet 的 nokmem Build Tags 来编译解决：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-s" data-lang="s"><span class="n">kubelet</span> <span class="n">GOFLAGS</span><span class="o">=</span><span class="s">&#34;-tags=nokmem&#34;</span>
</code></pre></td></tr></table>
</div>
</div><p>但需要注意，kubelet 版本需要在 v1.14 及以上。</p>
<h3 id="升级内核版本">升级内核版本</h3>
<p>升级 Linux 内核至 kernel-3.10.0-1075.el7 及以上就可以修复这个问题，详细可见 slab leak causing a crash when using kmem control group，其在发行版中 CentOS 7.8 已经发布。</p>
<h2 id="总结">总结</h2>
<p>经过内部讨论，由于种种原因（例如：Linux、Kubernetes 太低），我们选择了升级 Linux 版本，也就是 CentOS 8，这样子其内核版本就会到达至 4.x（cgroup 已经健壮了许多，且在 4.5 cgroup v2 已经 GA），相关问题已经修复，并同步设置 cgroup.memory=nokmem 即可解决/避免相关问题。</p>
<p>而在写下这篇文章时，我们可以看到 kmem accounting 的不少问题都已经被修复或提上日程，这对本次排查提供了相当大的便利，在确定问题的所在后根据 cgroup leak 沿着排查下去，基本都能看到大量的前人所经历过的 “挣扎”，大家若有兴趣，也可以跟着参考所提供的的链接做更一进步的深入了解。</p>
<p>但事实上，不管哪个 Linux 内核版本，都存在着或多或少的问题，需要做好适当的心理准备，否则就会遇到 “没上容器时好好的” 的窘境，查起问题更麻烦。</p>
<h2 id="后话">后话</h2>
<p>现在生产集群已经迁移完毕多日，通过近期的观察，已经确定了这个问题的修复和解决。这是原本的情况：</p>
<p><img src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210608123351.png" alt=""></p>
<p>新生产集群，经过数日后：</p>
<p><img src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210608123403.png" alt=""></p>
<p>通过对比，可以明确的看到，在原本的趋势图中，其在达到当时的内存高位后，即使在凌晨没有流量的情况下，容器内存也依然居高不下，纹丝不动，不会下降。</p>
<p>再反观最新的趋势图，在没有流量打入的情况下，容器内存开始下降，说明 Cahce 的自动回收已经正常的在运行了。</p>
<p>而自动回收的标准，一般常见于接近或达到全局内存水位的情况，系统会尽最大可能进行 Cache 的回收，以确保系统的正常运行：</p>
<p><img src="https://blog-1257321977.cos.ap-beijing.myqcloud.com/20210608123418.png" alt=""></p>
<p>至此，也就达到了修复这个问题的目的，解决了这一个长达两年的迷之内存漩涡。</p>
<h2 id="转载">转载</h2>
<p><a href="https://eddycjy.com/posts/why-container-memory-exceed/">为什么容器内存占用居高不下，频频 OOM</a></p>
<p><a href="https://eddycjy.com/posts/why-container-memory-exceed2/">为什么容器内存占用居高不下，频频 OOM（续）</a></p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">Forz</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2021-06-07
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/k8s/">k8s</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/go%E5%BA%94%E7%94%A8%E4%B8%BA%E4%BB%80%E4%B9%88vsz%E5%8D%A0%E7%94%A8%E8%BF%87%E9%AB%98/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Go应用为什么VSZ占用过高</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/k8s%E7%9A%84%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E5%AE%9E%E8%B7%B5/">
            <span class="next-text nav-default">K8s的日志采集实践</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  
    <script src="https://utteranc.es/client.js"
            repo="forz/forzblog.talk"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://github.com/utterance">comments powered by utterances.</a></noscript>

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  <a href="/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2017 - 
    2021<span class="heart"><i class="iconfont icon-heart"></i></span><span>Forz</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
